{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f758c821",
   "metadata": {},
   "source": [
    "## Getting started with prompt template and chat prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207b62f",
   "metadata": {},
   "source": [
    "### Use Prompt Template to create a template for string prompt\n",
    "By default, PromptTemplate uses Python's str.format syntax for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9f21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92010ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ec56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", temperature=0.5, google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780e422",
   "metadata": {},
   "source": [
    "A simple string based Prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e6241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the British weather get a therapist?\\n\\nBecause it was feeling a bit under the cloud.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-9c202a98-e47b-4034-bfc8-4dfa6cbb489c-0'\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt = prompt_template.format(adjective=\"sad\", content =\"British Weather\")\n",
    "\n",
    "respose = model.invoke(prompt)\n",
    "print(respose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8292dc",
   "metadata": {},
   "source": [
    "content='Why did the British weather get a therapist?\\n\\nBecause it was feeling a bit under the cloud.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-9c202a98-e47b-4034-bfc8-4dfa6cbb489c-0'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b8dca",
   "metadata": {},
   "source": [
    "### ChatPromtTemplate: The prompt to chat models is a list of chat messages.\n",
    "\n",
    "Each chat message is associated with content, and an additional parameter csalled role.\n",
    "\n",
    "For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916242b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc095e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='My name is Adem. I am a helpful AI assistant. I am good at:\\n\\n*   **Providing information:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n*   **Answering questions:** I can answer questions on a wide range of topics.\\n*   **Generating text:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\\n*   **Summarization:** I can summarize large amounts of text.\\n*   **Translation:** I can translate languages.\\n*   **Following instructions:** I can follow your instructions and complete your requests thoughtfully.\\n\\nHow can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-2b600ebd-589e-4f3a-892e-09a55f156773-0'\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, now are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt = chat_template.format_messages(name=\"Adem\", user_input=\"What is your name what are you good at ?\")\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee53837",
   "metadata": {},
   "source": [
    "content='My name is Adem. I am a helpful AI assistant. I am good at:\\n\\n*   \n",
    "**Providing information:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n*   \n",
    "**Answering questions:** I can answer questions on a wide range of topics.\\n*   \n",
    "**Generating text:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\\n*  \n",
    "**Summarization:** I can summarize large amounts of text.\\n*   \n",
    "**Translation:** I can translate languages.\\n*   \n",
    "**Following instructions:** I can follow your instructions and complete your requests thoughtfully.\\n\\nHow can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-2b600ebd-589e-4f3a-892e-09a55f156773-0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26805d5",
   "metadata": {},
   "source": [
    "### Various ways of formatting System/Human?AI prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dea700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='While lemongrass might not be your favorite, there are so many other exciting flavors to explore in the culinary world!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-396bddd2-34ab-47e4-b8ee-4d25e7adaec2-0'\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that re-writes the user's text to \" \n",
    "        \"sound more upbeat.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "])\n",
    "prompt = chat_template.format_messages(text=\"I don't like eating lemon grass\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db39ce",
   "metadata": {},
   "source": [
    "content='While lemongrass might not be your favorite, there are so many other exciting flavors to explore in the culinary world!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-396bddd2-34ab-47e4-b8ee-4d25e7adaec2-0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c6c4e",
   "metadata": {},
   "source": [
    "### Providing a Context along with the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369ad2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing a Context along with the Prompt\n",
    "prompt = \"\"\"\n",
    "Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer with \"I don't know\".\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful \n",
    "for developers building NLP enabled applications. These models can be accessed \n",
    "via Hugging Face's `transformers`library,via OpenAI using the `openai`library, \n",
    "and via Cohere using the`cohere`library. \n",
    "Question: Which libraries and model providers offer LLMs? \n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2aca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-cf6049cd-b091-4e95-8cbf-119dcf2fc00e-0'\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d83bbe",
   "metadata": {},
   "source": [
    "content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run-cf6049cd-b091-4e95-8cbf-119dcf2fc00e-0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b26ea",
   "metadata": {},
   "source": [
    "### Langchain's FreShotPromptTempate  caters to source knowledge input. The idea is to \"train\" the model on a few examples - we call this few-shot learning -  and these examples are given to the model within the prompt.\n",
    "\n",
    "The goal of few-shot prompt template are to dynamiclly select examples based on an input, and format the examples in a final prompt to provide for the model.\n",
    "\n",
    "#### Fixed Examples\n",
    "The most basic (and common) few-shot prompting technique is to use a fixed prompt example. this way you can select a chain, evaluate it , and avoid worrying about additional moving parts in production.\n",
    "\n",
    "The basic components of template are: \n",
    "\n",
    "examples - A list of dictionary examples to include in the final prompt.\n",
    "\n",
    "example_prompt: converts each example into 1 or more message through its format_messages method. \n",
    "\n",
    "A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d0acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create out examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"how are you?\",\n",
    "        \"answer\":\"I am doing well, thank you!\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"what is your name?\",\n",
    "        \"answer\":\"My name is Chatbot.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"what is your favorite color?\",\n",
    "        \"answer\":\"I like blue.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create the example template\n",
    "example_template = \"\"\"\n",
    "    \"User: {query}\"\n",
    "    \"AI: {answer}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba83b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into prefix and suffix\n",
    "# the prefix is out instructions\n",
    "prefix = \"\"\"\n",
    "    You are a helpful assistant. Answer the user's question in a friendly manner.\n",
    "    Use the examples below to help you understand how to respond.\n",
    "\"\"\"\n",
    "# the suffix is the user input\n",
    "suffix = \"\"\"\n",
    "    \"User: {query}\n",
    "    \"AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc8da4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ac9d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a helpful assistant. Answer the user's question in a friendly manner.\n",
      "    Use the examples below to help you understand how to respond.\n",
      "\n",
      "\n",
      "\n",
      "    \"User: how are you?\"\n",
      "    \"AI: I am doing well, thank you!\"\n",
      "\n",
      "\n",
      "\n",
      "    \"User: what is your name?\"\n",
      "    \"AI: My name is Chatbot.\"\n",
      "\n",
      "\n",
      "\n",
      "    \"User: what is your favorite color?\"\n",
      "    \"AI: I like blue.\"\n",
      "\n",
      "\n",
      "\n",
      "    \"User: What food should i eat tomorrow as I'm learing Langchain?\n",
      "    \"AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What food should i eat tomorrow as I'm learing Langchain?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "304351a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a fun question! Since you're learning Langchain, how about something that fuels your brainpower? Maybe a hearty salmon dish with roasted vegetables. Salmon is great for brain function, and the veggies will give you sustained energy. Enjoy your Langchain learning! ðŸ˜Š\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f3e364f5-91a9-45b7-9088-7dd6ca6be38a-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = few_shot_prompt_template | model\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdfc57d",
   "metadata": {},
   "source": [
    "AIMessage(content=\"That's a fun question! Since you're learning Langchain, how about something that fuels your brainpower? Maybe a hearty salmon dish with roasted vegetables. Salmon is great for brain function, and the veggies will give you sustained energy. Enjoy your Langchain learning! ðŸ˜Š\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f3e364f5-91a9-45b7-9088-7dd6ca6be38a-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d54d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
